# ğŸ–ï¸ Hand Gesture Control for movements and chats in Among Us using standard American Sign Language(ASL) Alphabets ğŸ®

> Control *Among Us* with hand gestures using real-time webcam input and a custom-trained ASL Alphabets classifier (trained on MLP, with 5138 samples captured by me).

---

## ğŸ“½ï¸ Demo  

Watch NEURA in action:

â–¶ï¸ [LinkedIn demo post](https://www.linkedin.com/posts/saumya-gupta-4385452a4_opencv-amongus-mediapipe-activity-7338291179605630976-AwcK?utm_source=share&utm_medium=member_desktop&rcm=ACoAAEloJCwBPPaMNpyq0mm_XSFMSNrM7BK6LVE)

ğŸ“ [Google Drive video folder](https://drive.google.com/drive/folders/1jppYxLMYDGygf4AumggTf-Fpdl5L1-H4?usp=drive_link)

#### âš ï¸ please note

..so this project is a demo only,  not a product; and is created just for experimenting.

- designed for the web version of among us
- crewmate tasks require mouse interactions, which are not currently supported by this gesture system.
- this system is optimized for movement and action controls (e.g., walking, venting, killing, reporting, typing).


## Features  
```
- âœ… real-time ASL gesture recognition (28 gestures - 26 alphabets(standard ASL) + space(custom) + enter(custom)) 
- ğŸ”„ gesture to text conversion in realtime in chats 
- ğŸ® joystick-style finger-based movement (WASD)     
- ğŸ”„ gesture cooldowns to prevent rapid repeat actions  
- ğŸ” toggle between joystick and gesture mode  
- ğŸ¯ custom controls for Among Us:
  - `P` â†’ Kill + Vent  
  - `Y` â†’ Report  
  - `ENTER` â†’ Report meeting  
  - `F` â†’ Toggle Joystick on/off  
```
---

## ASL Gesture Recognition Model  

### ğŸ“Š Model Architecture
- **Classifier**: `MLPClassifier` (Multi-Layer Perceptron)  
- **Hidden Layer**: 1 layer with 100 neurons  
- **Max Iterations**: 300  
- **Preprocessing**:
  - `StandardScaler` for normalization  
  - `LabelEncoder` for class encoding  

### ğŸ“ Dataset
- **Size**: 5,138 samples  
- **Classes**: 28  
  - Letters: `A` to `Z`  (currently, letters J and Z have static samples only since dynamic are not yet supported,tho i am planning to add this in future)
  - Special: `"SPACE"` and `"ENTER"`  
- **Collected using**: Mediapipe hand landmarks  

### ğŸ§ª Training Results
- **Test Samples Evaluated**: 1,028  
- **Accuracy**: `98%`  
- **Macro F1-Score**: `1.00`  
- **Weighted F1-Score**: `1.00`  

#### ğŸ“Œ Per-Class Performance

![Classification Report](visuals/Classification_Report.png)  
*Classification report showing precision, recall, and F1 for each ASL label.*

| Class | Performance |
|-------|-------------|
| âœ… Perfect (24/28) | A, C, D, E, F, G, H, I, J, K, L, O, P, Q, R, S, T, U, V, X, Y, Z, SPACE, ENTER |
| âš ï¸ slight drop | M (F1 = 0.97), N (Precision = 0.94), W (Recall = 0.97) |

![ASL Letter Confusion Matrix](visuals/ASL_letter_Confusion_Matrix.png)  
*Confusion matrix for ASL letter classification.*

---

## ğŸ› ï¸ How It Works

### ğŸ¥ Input
- Captures webcam feed  
- Uses **MediaPipe** to detect hand landmarks  
- extracts 63 features per frame (21 landmarks Ã— 3 coordinates)  

### ğŸ§  Gesture Prediction
- preprocesses input via `StandardScaler`  
- predicts gesture using pre-trained `MLPClassifier`  
- adds debounce logic and cooldown timers to prevent misclassification  

### ğŸ•¹ï¸ Joystick Control
- Fingertip (index) position controls movement inside a dynamic virtual D-pad  
- Idle zone in center to avoid accidental key presses  
- Keys Held: `W`, `A`, `S`, `D` for movement  

### âœŠ Gesture Actions
| Gesture | Action |
|---------|--------|
| `P`     | triggers both Kill (`E`) and Vent (`Space`) |
| `Y`     | triggers Report (`R`) |
| `ENTER` | triggers meeting report |
| `F`     | toggles joystick movement |
| Aâ€“Z     | typed directly as lowercase text |

---

## ğŸ“‚ Folder Structure  
```
amongus_hand_control/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ asl_data.csv
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ asl_mlp_model.pkl
â”‚   â”œâ”€â”€ asl_label_encoder.pkl
â”‚   â””â”€â”€ asl_scaler.pkl
â”‚
â”œâ”€â”€ visuals/
â”‚   â”œâ”€â”€ Classification_Report.png
â”‚   â”œâ”€â”€ ASL_letter_Confusion_Matrix.png
|
â”œâ”€â”€ movement.py             # main
â”œâ”€â”€ asl_check.py
â”œâ”€â”€ record_asl_data.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ venv
â”œâ”€â”€ .gitignore
```

---

## ğŸ“¦ Requirements  

Install the following Python libraries:

```bash
pip install opencv-python mediapipe pyautogui numpy scikit-learn joblib
```
then run : 

```bash
python movement.py
```
---

## ğŸ® Tested On  

- **Game**: Among Us
- **Platform**: PC (Windows)  
- **Python Version**: 3.8+  

---

## ğŸ™Œ Credits

- Built by: *Saumya Gupta*  
- Hand tracking: [MediaPipe](https://mediapipe.dev/)  
- Game automation: [PyAutoGUI](https://pyautogui.readthedocs.io/)  
- Game: [Among Us Â© Innersloth](https://www.innersloth.com/)  

---

## ğŸ“Œ Future Improvements 
```
- Multi-hand support (dual gesture input)  
- Mouse control with thumb + index pinch to perform crewmate tasks
- Visual cooldown indicator on screen  
- addition of dynamic movements of ASL 'J' and 'Z' 
```
